---
title: "Tutorial for densify"
author: "Anna Graff"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: sources.bib
vignette: >
  %\VignetteIndexEntry{Tutorial for densify}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# 1. Introduction

The `densify` package provides a procedure to generate denser subsets of input data frames according to varying user-defined parameters. The densification process is split up into three functions, `densify_steps`, `densify_score` and `densify_prune`. This tutorial guides users through all functions and parameters to demonstrate their effect on densification to help users select settings according to their needs. 

The data used in this tutorial are the default data provided in the package (the language-variable matrix `wals` obtained from the World Atlas of Language Structures (WALS [@wals]) and the language taxonomy `glottolog_languoids` provided by Glottolog, Version 4.8 [@glottolog]).

# 2. Matrix densification
## 2.0. Preparing input

The input for densification needs to be a data frame with rows representing observations or taxa (e.g., languages) and columns representing variables (in linguistic typological databases, these are also often referred to as "features", "characters" or "parameters"). Taxon (or other observation) names must be set as the row names of the data frame, and variable names must be set as the column names. Any cells with empty entries, not applicable or question marks must be coded as `NA`. 

If the densification process should be sensitive to the taxonomic structure of the observations, a flat taxonomy must be provided, listing every taxon present in the initial data frame along with all the nodes connecting it to the root. Such a taxonomy can be generated with the `build_flat_taxonomy_matrix` function, if all nodes and tips are provided alongside each of their parent nodes. For generating a language taxonomy, the "languoids.csv" file from Glottolog can be used directly.

<!--# See my comment on the paper: what would user do if their taxonomy is in a more standard format such as newick? -->

```{r packages, echo = TRUE, warnings = FALSE, message = FALSE}
# load packages
library(densify)
library(tidyverse)
```

```{r taxonomy, echo = TRUE}
# exclude languages from wals that are not attributable to glottolog taxonomy
wals <- wals[which(rownames(wals) %in% glottolog_languoids$id), ]

# ensure all missing and non-applicable information is coded as NA
wals[wals==""] <- NA
wals[wals=="?"] <- NA
wals[wals=="NA"] <- NA

# create glottolog taxonomy
taxonomy_matrix <- build_flat_taxonomy_matrix(id = glottolog_languoids$id, 
                                              parent_id = glottolog_languoids$parent_id)
```

The initial (full) dataset provided by WALS encompasses data on 192 linguistic variables, coded for 2496 languages (representing 317 families) that are attributable to Glottolog. 

```{r unpruned wals summary statistics, echo = TRUE}
# number of languages
nrow(wals) 
# number of families
taxonomy_matrix %>% filter(id %in% rownames(wals)) %>% select(level1) %>% unique() %>% nrow()
# number of variables
ncol(wals) 
```

However, in this data matrix each variable is coded for a distinct set of languages, such that **coding density**, which we define as the proportion of non-NA, non-empty and non-? cells, is patchy for both languages and variables.

```{r patchy coding for languages and variables, echo=FALSE, fig.show="hold", out.width="40%", fig.width=4.5, fig.height=3}
par(mar = c(4, 4, .1, .1))

ggplot(data.frame(density=apply(wals, 1, function(x) (length(na.omit(x))))/ncol(wals)), aes(x=density))+
  geom_histogram(color="black", fill="cadetblue2", bins=20)+
  theme_bw()+
  labs(title="Unpruned WALS", 
       x="coding density per row (language)",
       y="frequency")

ggplot(data.frame(density=apply(wals, 2, function(x) (length(na.omit(x))))/nrow(wals)), aes(x=density))+
  geom_histogram(color="black", fill="forestgreen", bins=20)+
  theme_bw()+
  labs(title="Unpruned WALS",
       x="coding density per column (variable)",
       y="frequency")
```

The overall coding density of the input data frame is 15.7%.

```{r overall coding density input data frame, echo = TRUE}
sum(!is.na(wals))/(ncol(wals)*nrow(wals))
```

## 2.1. Matrix densification: iterative pruning using `densify_steps`

A straightforward method to find a denser sub-matrix within a sparse super-matrix is to remove all rows and columns with coding densities below a given threshold. However, removing a row subtly changes all column densities and vice versa, so a more cautious method is to iteratively remove rows and columns, always updating computations regarding the coding densities of all remaining rows and columns once a row or column is removed. The function `densify_steps` accomplishes this: It takes the original data matrix, iteratively removes rows and columns according to specifiable parameters described below, and logs the characteristics of the matrix after each iteration. The output of the function is an iteration log, which is parsed by the next function, `densify_score` (see below), to determine the optimal number of iterations according to user-specified criteria post-hoc.

### 2.1.1. High-level description of `densify_steps`

Based on the sparsely encoded data frame provided by the user, `densify_steps` defines an **encoding matrix** of the same dimensions, containing zeroes for NAs and ones elsewhere. Using this encoding matrix, it computes a **quality score for each row and each column** according to user-specifiable settings, identifies the row and/or column with the lowest quality score (randomly sampling one row or column if there are ties) and removes it. After the first row or column is removed, the function re-establishes the quality scores of the resulting sub-matrix, identifies which row and/or column scores worst in the new matrix, and in turn removes it. This process is repeated for the number of iterations specified in a user-defined function parameter `max_steps` or until the matrix is empty. Key characteristics (e.g. number of rows, number of columns, identity of removed rows, identity of removed columns, etc.) of all sub-matrices produced in the pruning process are documented in an iteration log, which forms the output of the function.

There are several ways with which quality scores for rows and columns can be computed. The approach adopted distinguishes between what will be referred to as absolute and weighed coding densities. **Absolute coding densities** denote the (raw) proportion of coded cells in a row or column. If a row is coded for 10% of the columns in the matrix, its absolute coding density is thus 0.1. Several rows may be coded for the same absolute proportion of columns, but the identity of the columns for which such rows are coded may well be distinct - some rows may be coded for columns which are well-coded, while others may be coded for columns coded only for few rows. Such subtle (or less subtle) differences among rows coded for the same absolute proportion of columns should be taken into account when establishing the quality scores for rows and columns in the iterative pruning approach adopted here, such that among those sparsely coded rows (or columns, respectively) coded for the same proportion of columns (or rows, respectively), the ones coded for less well-coded columns are removed first.

Thus, to determine the quality score of a column (variable), `densify_steps` computes the weighted column coding density. To determine the quality score of a row (language), on the other hand, one of three possible **mean measures** described in detail below is computed that takes into account (1) the absolute row coding densities, (2) the weighted row coding densities and - if specified - (3) the contribution of a specific row to the taxonomic diversity of the sample.

Row and column quality scores are treated differently for two reasons. First, only rows can have taxonomic structure. Second, in most linguistic typological research applications, the focus of densification resides chiefly on pruning away rows (languages/taxa/observations) rather than columns, given that the former are usually substantially more numerous, and including absolute coding densities in the row quality score primarily has the effect of penalizing scarcely-coded rows in the matrix more strongly. 

Removing rows can have effects not only on coding densities of columns, but also on their informativeness: It is possible for the iterative removal of rows to slowly deplete a variable state, such that a variable (column) no longer displays sufficient or any variability (i.e., a column may still be coded for a substantial number of rows but all remaining coded rows might have the value "FALSE", because all rows with the value "TRUE" have been pruned away). Thus, after each row removal `densify_steps` assesses whether any variables have become uninformative according to a user-defined threshold denoting how many observations must be present in the second-largest variable state, and removes any such column(s) alongside the row in the same iteration. Conversely, it is computationally possible that the removal of a specific column results in a row no longer exhibiting any data points (because it was only coded for the column that was removed). Thus, after each column removal the algorithm assesses whether any rows have become empty and removes any such row(s) alongside the column in the same iteration.

### 2.1.2. The parameters for `densify_steps`
The function `densify_steps` requires the user to define a number of parameters, most of which modulate the way the row quality scores are computed at each iteration. This section describes and discusses all parameters available to fine-tune the densification process. 

#### Mandatory parameters
`densify_steps` takes the following mandatory arguments:

-   `original_data`: The data frame to be pruned, in the format described above. The default data frame used is WALS.
-   `max_steps`: An integer specifying the maximum number of iterations performed. The default number of iterations is set to `1`. We recommend setting max_steps to the maximum possible number of iterations conceivable for a given input matrix (i.e. `nrow(original_data)+ncol(original_data)-2`).  <!--# BB: why not make the recommended number the default? that's common practice:::: ANNA: I wouldn't know how to do this, because "original_data" has to be provided as a separate parameter, and I dont think one can recruit the value of another parameter to generate the default of another parameter in the same variable? 
ML: I agree with Anna. Moreover, 1 is just safer: You don't want to have to stop the script if you accidentally forgot this parameter. -->
-   `variability_threshold`: An integer specifying how many observations/taxa the second-most-frequent variable state of any variable must contain for the variable to be maintained in the matrix. The default set to `1`, guaranteeing that a variable must display at least some variability (at least one observation must be different from all others). <!--# BB: pls try to unpack what this means, also consider what a sensible default would be. It's not clear to me::ANNA: I hope this has become clearer, with the re-written section above? -->
-   `taxonomy`: A logical parameter denoting whether the taxonomic structure of rows shall be taken into account in the pruning process or not. The default setting is `FALSE`, because taxonomies may not always be available.
-   `mean_type`: This parameter specifies how the measure denoting row quality is computed. It can be one of the following three mean-types: `arithmetic`, `geometric`, `log_odds`. See below for more details. The default setting is `log_odds`. <!--# BB: it won't be obvious in what sense log odds is a kind of mean, better introduce with notion of score above. Perhaps just a matter of terminology but can easily puzzle readers. It will help to introduce the notions "encoding matrix", "coding density", "quality score", "mean" step by step, defining th terms clearly and illustrate with a toy example. ANNA: I hope this has become clearer, with the re-written section above?-->
-   `verbose`: A logical parameter denoting whether the function output should be concise or detailed. The default setting is for the output to be concise (`FALSE`).

The following parameters are conditionally mandatory:

-  `taxonomy_matrix`: The taxonomy matrix encompassing all taxa represented in original_data, in the format described above. If the parameter `taxonomy = TRUE`, it is mandatory. Otherwise, it is optional. The default setting is `NULL`, denoting that no taxonomy matrix is provided.
<!--# BB:shouldn't this be FALSE?:: ANNA: No, because it is not a logical but a data frame that must is provided -->

-  `taxonomy_weight`: This parameter specifies the weight factor attributed to contribution to taxonomic diversity in the computation of the log_odds mean of rows. It must lie between 0 and 1 (excluding these values) and is mandatory if `mean_type = "log_odds"` and if `taxonomy = TRUE`. Otherwise, it is undefined. The default value is `0.99`. See below for effects in varying weights for the log_odds mean.  <!--# BB:can you say what that intuition this captures?:: ANNA: is this ok?-->
-  `coding_weight`: This parameter specifies the weight factor attributed to absolute and weighted coding densities in the computation of the log_odds mean of rows. It must lie between 0 and 1 (excluding these values) and is mandatory if `mean_type = "log_odds"`. Otherwise, it is undefined. The default value is `0.99`. See below for effects in varying weights for the log_odds mean.

While it is possible to run `densify_steps` with several parameter settings on the same input data frame, we strongly recommend motivating parameter settings with the desired characteristics of the output data frame (e.g. how strongly should taxonomic diversity be weighted in relation to coding density of rows?) in mind.

#### Parameter `variability_threshold`
While it is logically conceivable that one might want to include variables in a dataset displaying no variability whatsoever among the coded taxa/observations (`variability_threshold = 0`), such variables are uninteresting for most analyses. 

Setting the parameter `variability_threshold = 1` (the default value) ensures that such variables - whether they are present in the input data matrix or whether they come into being as a result of removing rows - are immediately removed in the pruning process. 

For certain comparative analyses, it may be reasonable to set the threshold to an integer n > 1, because such settings ensure that each variable contains at least two values including n or more taxa/observations.  <!--# BB:please explain up front what the numbers stand for, it sounds like it's the number of types/values. If so say so; if not, make sure readers won't misunderstand:: ANNA Has this become clearer now?-->

#### Parameters `taxonomy` and `taxonomy_matrix`

The logical parameter `taxonomy` denotes whether taxonomic structure among the observations/entities represented in rows should contribute to determining the quality score of rows in each iteration. It is `FALSE` by default because it requires a taxonomy matrix, which is not always applicable or available. 

A taxonomy matrix in the format produced by `build_flat_taxonomy_matrix` must be provided under the parameter `taxonomy_matrix` whenever the parameter `taxonomy = TRUE`. It may (but does not have to) be provided when `taxonomy = FALSE`. Providing a taxonomy matrix when `taxonomy = FALSE` results in taxonomic structure not being considered for computing row quality scores, but a taxonomic diversity score (the Shannon entropy of the highest taxonomic level in the taxonomy) being computed and logged for each sub-matrix in the iteration_log produced by `density_steps`. This way, disconsidering taxonomic diversity in the pruning process does not preclude taxonomic diversity from being considered for identifying the optimal number of iterations using `densify_prune` later (see below).  <!--# BB: unclear, please unpack, step by step:: ANNA: Has this become clearer? -->

#### Parameters `mean_type`, `taxonomy_weight` and `coding_weight`

`densify_steps` offers three methods of computing the mean of row-wise absolute and weighted coding densities as well as contribution to taxonomic diversity of the sample where applicable: Possible values are `arithmetic`, `geometric`, or `log_odds`. 

The arithmetic and geometric means operate as conventionally understood in mathematics: the arithmetic mean utilizes the `base::mean` function, while the geometric mean involves the n-th root of the product. For `mean_type = "arithmetic"` and `mean_type = "geometric"`, the respective means of (1) the absolute and (2) weighted coding densities are computed for each row, if `taxonomy = FALSE`. If `taxonomy = TRUE`, the respective means of the (1) absolute coding density, (2) the weighted coding density and (3) the contribution to taxonomic diversity are computed for each row.

The log odds mean is computed by taking the logit transform of all inputs, then taking the arithmetic mean and then transforming back. In R, the logit function is available as `qlogis()` and the inverse logit as `plogis()`. We thus compute:

```{r, eval=FALSE}
log_odds_mean = plogis(mean(qlogis(inputs)))
```

As for the other `mean_type` values, the value `log_odds` specifies for the log odds means of the (1) absolute and (2) weighted coding densities to be computed if `taxonomy = FALSE` and the log odds means of the (1) absolute coding density, (2) the weighted coding density and (3) the contribution to taxonomic diversity to be computed if `taxonomy = TRUE`. However, `mean_type = log_odds` requires specifying the aforementioned additional parameter `coding_weight` and, if `taxonomy = TRUE`, also a separate parameter `taxonomy_weight`. In addition to serving to avoid possible computations like qlogis(0)+qlogis(1) = -Inf+Inf = NaN, the weight factors allow for modulating the behaviour of the log odds mean (see below) and can be set separately for the absolute and weighted coding density (`coding_weight`) and contribution to taxonomic diversity (`taxonomy_weight`) to bias the weight of taxonomic diversity relative to the weight of codedness in the computation of the row means and ultimately in the iterative pruning process.

To illustrate the different effects of the three mean types available, the first plot below traces the mean of a constant 0.5 and a value x between 0 and 1 of mean types `arithmetic`, `geometric` and `log_odds`. Note how the log odds mean strongly skews towards the values 0 and 1 once x approaches these extremes, while the geometric mean does so only towards 0. If users intends to strongly weight extremely low (approaching 0) or extremely high (approaching 1) values regarding either contribution to taxonomic diversity or codedness, they should thus favour the log odds or possibly the geometric mean for `densify_steps`.

The second plot illustrates how different weight factors (here: 0.5, 0.75, 0.95 and 0.99) modulate the behaviour of the log odds mean of a constant 0.5 and all possible values of x.

```{r mean comparison, eval = TRUE, echo = FALSE, fig.show="hold", out.width="45%", fig.width=6, fig.height=4}
# Define a function to calculate the arithmetic mean
arithmetic_mean <- function(x) {
  (1/2 + x) / 2
}

# Define a function to calculate the geometric mean
geometric_mean <- function(x) {
  sqrt(1/2 * x)
}

# Define a function to calculate the log odds mean (lom)
lom <- function(x) {
  plogis(mean(qlogis(c(1/2, x))))
}

# Create a sequence of x values from 0 to 1:
x_values <- seq(0, 1, by = 0.001)

# Calculate the different means for each x value
arithmetic_mean_values <- sapply(x_values, arithmetic_mean)
geometric_mean_values <- sapply(x_values, geometric_mean)
lom_values <- sapply(x_values, lom)
lom_values_99 <- sapply(x_values, function(x) lom(x * 0.99))
lom_values_95 <- sapply(x_values, function(x) lom(x * 0.95))
lom_values_75 <- sapply(x_values, function(x) lom(x * 0.75))
lom_values_50 <- sapply(x_values, function(x) lom(x * 0.5))

values <- data.frame(x_values = x_values,
                     arithmetic_mean_values = arithmetic_mean_values,
                     geometric_mean_values = geometric_mean_values,
                     lom_values = lom_values,
                     lom_values_99 = lom_values_99,
                     lom_values_95 = lom_values_99,
                     lom_values_75 = lom_values_75,
                     lom_values_50 = lom_values_50,
                     one_half = rep(1/2,1001))

par(mar = c(4, 4, .1, .1))
# Create first plot:
ggplot(values, aes(x = x_values)) +
  theme_bw() +
  geom_line(aes(y = arithmetic_mean_values, color = "Arithmetic Mean"), show.legend = TRUE) + 
  geom_line(aes(y = geometric_mean_values, color = "Geometric Mean"), show.legend = TRUE) +
  geom_line(aes(y = lom_values, color = "Log Odds Mean"), show.legend = TRUE) +
  geom_line(aes(y = one_half, color = "1/2"), linetype = "twodash", show.legend = TRUE) +
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_continuous(expand = c(0, 0)) +
  scale_color_manual(values = c(
    "Arithmetic Mean" = "purple",
    "Geometric Mean" = "steelblue",
    "Log Odds Mean" = "black",
    "1/2" = "red"
  )) +
  ggtitle("Comparison of Means") +
  labs(x = "x", y = "Means", color = "Mean variations")

# Create second plot:
ggplot(values, aes(x = x_values)) +
  theme_bw() +
  geom_line(aes(y = lom_values, color = "lom(x)"), show.legend = TRUE) + 
  geom_line(aes(y = lom_values_99, color = "lom(x*0.99)"), show.legend = TRUE) +
  geom_line(aes(y = lom_values_95, color = "lom(x*0.95)"), show.legend = TRUE) +
  geom_line(aes(y = lom_values_75, color = "lom(x*0.75)"), show.legend = TRUE) +
  geom_line(aes(y = lom_values_50, color = "lom(x*0.5)"), show.legend = TRUE) +
  geom_line(aes(y = one_half, color = "1/2"), linetype = "twodash", show.legend = TRUE) +
  scale_x_continuous(expand = c(0, 0)) + 
  scale_y_continuous(expand = c(0, 0)) +
  scale_color_manual(values = c(
    "lom(x)" = "black", 
    "lom(x*0.99)" = "blue", 
    "lom(x*0.95)" = "green", 
    "lom(x*0.75)" = "orange", 
    "lom(x*0.5)" = "pink", 
    "1/2" = "red"
  )) +
  ggtitle("Log Odds Mean Variations") +
  labs(x = "x", y = "Log Odds Mean (lom)", color = "LOM variations")
```

The parameters `mean_type`, `coding_weight` and `taxonomy_weight` thus modulate the pruning process and can (and should) be adjusted to meet the needs of the user. 

### 2.1.3.  Preparing examples with varying parameters

The following 16 implementations of `densify_steps` vary selected key parameters discussed above. Their effect on the densification process will be illustrated below.

```{r densify_steps generate iteration_logs, eval = FALSE}
# the following parameters remain constant throughout all runs:
max_steps <- nrow(wals)+ncol(wals)-2
variability_threshold <- 3

# arithmetic mean, taxonomy = TRUE and taxonomy = FALSE
set.seed(1234)
iteration_log_arithmetic_taxtrue_seed1234 <- 
  densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold, 
                mean_type = "arithmetic", 
                taxonomy = T, 
                taxonomy_matrix = taxonomy_matrix)

set.seed(4321)
iteration_log_arithmetic_taxfalse_seed4321 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold, 
                mean_type = "arithmetic", 
                taxonomy = F, 
                taxonomy_matrix = taxonomy_matrix)

# geometric mean, taxonomy = TRUE and taxonomy = FALSE
set.seed(5678)
iteration_log_geometric_taxtrue_seed5678 <- 
  densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold, 
                mean_type = "geometric", 
                taxonomy = T, 
                taxonomy_matrix = taxonomy_matrix)

set.seed(8765)
iteration_log_geometric_taxfalse_seed8765 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold, 
                mean_type = "geometric", 
                taxonomy = F, 
                taxonomy_matrix = taxonomy_matrix)

# log_odds mean, varying coding_weight and taxonomy_weight (where applicable) 
# for both taxonomy = TRUE and taxonomy = FALSE
set.seed(1111)
iteration_log_logodds_taxtrue_099099_seed1111 <- 
  densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.99, 
                coding_weight = 0.99)

set.seed(2222)
iteration_log_logodds_taxtrue_095095_seed2222 <-   
  densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.95, 
                coding_weight = 0.95)

set.seed(3333)
iteration_log_logodds_taxtrue_090090_seed3333 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.90, 
                coding_weight = 0.90)

set.seed(4444)
iteration_log_logodds_taxtrue_050050_seed4444 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.50, 
                coding_weight = 0.50)

set.seed(1122)
iteration_log_logodds_taxtrue_099095_seed1122 <-
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.99, 
                coding_weight = 0.95)

set.seed(2211)
iteration_log_logodds_taxtrue_095099_seed2211 <-
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.95, 
                coding_weight = 0.99)

set.seed(1133)
iteration_log_logodds_taxtrue_099090_seed1133 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.99, 
                coding_weight = 0.90)

set.seed(3311)
iteration_log_logodds_taxtrue_090099_seed3311 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds",
                taxonomy = T,
                taxonomy_matrix = taxonomy_matrix, 
                taxonomy_weight = 0.90, 
                coding_weight = 0.99)

set.seed(9999)
iteration_log_logodds_taxfalse_099_seed9999 <- 
  densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds", 
                taxonomy = F, 
                taxonomy_matrix = taxonomy_matrix, 
                coding_weight = 0.99)

set.seed(8888)
iteration_log_logodds_taxfalse_095_seed8888 <- 
    densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds", 
                taxonomy = F, 
                taxonomy_matrix = taxonomy_matrix, 
                coding_weight = 0.95)

set.seed(7777)
iteration_log_logodds_taxfalse_090_seed7777 <- 
      densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds", 
                taxonomy = F, 
                taxonomy_matrix = taxonomy_matrix, 
                coding_weight = 0.90)
set.seed(6666)
iteration_log_logodds_taxfalse_050_seed6666 <- 
      densify_steps(original_data = wals, 
                max_steps = max_steps, 
                variability_threshold = variability_threshold,
                mean_type = "log_odds", 
                taxonomy = F, 
                taxonomy_matrix = taxonomy_matrix, 
                coding_weight = 0.50)
```

```{r densify_steps read in iteration_logs, eval = TRUE, echo = FALSE}
# to not rerun all densify_steps() runs, read in the stored iteration_log files
iteration_log_arithmetic_taxtrue_seed1234 <- read.csv("vignette_iteration_logs/iteration_log_arithmetic_taxtrue_seed1234.csv") %>% select(-X)
iteration_log_arithmetic_taxfalse_seed4321 <- read.csv("vignette_iteration_logs/iteration_log_arithmetic_taxfalse_seed4321.csv") %>% select(-X)
iteration_log_geometric_taxtrue_seed5678 <- read.csv("vignette_iteration_logs/iteration_log_geometric_taxtrue_seed5678.csv") %>% select(-X)
iteration_log_geometric_taxfalse_seed8765 <- read.csv("vignette_iteration_logs/iteration_log_geometric_taxfalse_seed8765.csv") %>% select(-X)
iteration_log_logodds_taxtrue_099099_seed1111 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_099099_seed1111.csv") %>% select(-X)
iteration_log_logodds_taxtrue_095095_seed2222 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_095095_seed2222.csv") %>% select(-X)
iteration_log_logodds_taxtrue_090090_seed3333 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_090090_seed3333.csv") %>% select(-X)
iteration_log_logodds_taxtrue_050050_seed4444<- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_050050_seed4444.csv") %>% select(-X)
iteration_log_logodds_taxtrue_099095_seed1122 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_099095_seed1122.csv") %>% select(-X)
iteration_log_logodds_taxtrue_095099_seed2211 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_095099_seed2211.csv") %>% select(-X)
iteration_log_logodds_taxtrue_099090_seed1133 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_099090_seed1133.csv") %>% select(-X)
iteration_log_logodds_taxtrue_090099_seed3311 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxtrue_090099_seed3311.csv") %>% select(-X)
iteration_log_logodds_taxfalse_099_seed9999 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxfalse_099_seed9999.csv") %>% select(-X)
iteration_log_logodds_taxfalse_095_seed8888 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxfalse_095_seed8888.csv") %>% select(-X)
iteration_log_logodds_taxfalse_090_seed7777 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxfalse_090_seed7777.csv") %>% select(-X)
iteration_log_logodds_taxfalse_050_seed6666 <- read.csv("vignette_iteration_logs/iteration_log_logodds_taxfalse_050_seed6666.csv") %>% select(-X)
```

## 2.2. Matrix densification: determining the optimal number of iterations using `densify_score`

The output from `densify_steps` allows for users to easily compare and retrieve all sub-matrices produced in the iterative pruning process. Naturally, the sub-matrices generated after more iterations are denser than those after only few pruning steps. However, they also contain less data. There is thus a trade-off between the proportion of coded data and the number of available data points that the users can exploit to determine the optimal number of iterations. If taxonomic diversity is relevant to users, it provides an addional criterion that can modulate which number of iterations is optimal: If the input is highly taxonomically unbalanced and taxonomic structure is considered for pruning, the taxonomic diversity of the taxa provided in rows (computed as the Shannon entropy of the highest taxonomic level of all taxa present) will tend to increase throughout the initial pruning phase until it levels off and subsequently decreases.

The function `densify_score` takes the iteration log from `densify_steps` as an input and uses it to compare the sub-matrices resulting from each iteration by computing a **matrix quality score** via user-defined exponents that can bias the quality score towards the desired axes in these trade-offs. The function identifies the iteration at which the score is maximized (selecting the earlier iteration in the unlikely case of ties), accompanied by a plot of all quality scores if specified by the user. The higher the optimal number of iterations, the more the resulting densified matrix will deviate from the input matrix.

<!--# BB:here it would be good to first give an overall explanation of the trade-offs between the various ways of setting the parameters: more data but then also more NAs; more taxonomic diversity, less data, etc. ANNA: Hope this is better now? -->

The quality score is the product of five values, each modifiable by an exponent: (1) the proportion of coded data in the matrix overall, (2) the absolute number of non-missing data points in the matrix, (3) the absolute coding density of the least well-coded row, (4) the absolute coding density of the least well-coded column, and (5) the taxonomic diversity of the taxa provided in rows (computed as the Shannon entropy of the highest taxonomic level of all taxa present; applicable only if a taxonomy matrix is provided).

Thus, the following parameters are available to modulate the weight of each of these values to identify the optimal sub-matrix:

-   `exponent_prop_coded_data`: By increasing this parameter, the weight of the proportion of coded data in the overall matrix is increased for the quality score. Thus, denser matrices are given higher quality scores, all other things equal. This parameter is obligatory and set to `1` by default.
-   `exponent_available_data_points`: By increasing this parameter, the weight of the absolute number of data points present in the matrix is increased for the quality score. Thus, larger matrices (with more rows and columns and thus data points) are given higher quality scores, all other things equal. This parameter is obligatory and set to `1` by default.
-   `exponent_lowest_taxon_coding_score`: By increasing this parameter, the weight of the coding density of the most sparsely-coded row in the matrix is increased for the quality score. Thus, matrices including rows with low coding densities are given lower quality scores, all other things equal. This parameter is optional, with no default value.
-   `exponent_lowest_variable_coding_score`: By increasing this parameter, the weight of the coding density of the most sparsely-coded column in the matrix is increased for the quality score. Thus, matrices including columns with low coding densities are given lower quality scores, all other things equal. This parameter is optional, with no default value.
-   `exponent_taxonomic_diversity`: By increasing this parameter, the weight of the taxonomic diversity of included taxa in the matrix is increased for the quality score. Thus, matrices with higher taxonomic diversities are given higher quality scores, all other things equal. This parameter is optional and only conditionally applicable (if a taxonomy matrix is provided), with no default value.


Each defined exponent must be a positive number. Setting an exponent to `0` corresponds to disregarding the corresponding value in the computation of the quality score. The exponents' magnitude in relation to one another translates into the relative weight given to each corresponding value in the computation of quality scores, i.e. increasing an exponent by factor 2 results in the corresponding value weighing twice as much, all other things equal.

As such, in the example settings below, the following are equivalent: (1), (2) and (3); (4) and (5). 

1.  exponent_prop_coded_data = 1, exponent_available_data_points = 1, exponent_lowest_taxon_coding_score = 0, exponent_lowest_variable_coding_score = 0, exponent_taxonomic_diversity = 0
2.  exponent_prop_coded_data = 1, exponent_available_data_points = 1 
3.  exponent_prop_coded_data = 2, exponent_available_data_points = 2 
4.  exponent_prop_coded_data = 0.5, exponent_available_data_points = 1 
5.  exponent_prop_coded_data = 1, exponent_available_data_points = 2

Users can run `densify_score` multiple times on the same iteration_log log using varying exponents to fine-tune the settings according to their needs. We recommend the choice of exponents to be motivated by characteristics of the input data frame as well as the desired characteristics of the output data frame in mind. 

For instance, if coding sparsity of the input matrix mainly manifests itself in rows but not columns, and an output matrix with low row coding density is undesirable, this should be expressed by including the parameter `exponent_lowest_taxon_coding_score` (e.g. `exponent_lowest_taxon_coding_score = 1`) and possibly excluding the parameter `exponent_lowest_variable_coding_score`. 

Other users may have an extremely sparse matrix and merely seek to strongly densify it, irrespective of taxonomic structure in rows. They would seek to include `exponent_lowest_taxon_coding_score`, `exponent_lowest_variable_coding_score`, possibly even increasing `exponent_prop_coded_data` relative to `exponent_available_data_points`. 

In another example, coding densities may be high and roughly even across rows and columns, and the motivation for pruning resides mainly in selecting a taxonomically diverse sub-sample for an analysis. In such a case, the parameter `exponent_taxonomic_diversity` would be included, while the parameters `exponent_lowest_taxon_coding_score` and `exponent_lowest_variable_coding_score` might be omitted, and the user may even consider to lower the parameter `exponent_available_data_points` relative to `exponent_prop_coded_data`.

To appreciate the effect of the parameters of `densify_score`, compare the following quality score plots and optimum iterations retrieved using the following parameter settings on the same iteration log file (obtained from an implementation of `densify_steps` using the settings `mean = "log_odds", taxonomy = TRUE, taxonomy_weight = 0.99, coding_weight = 0.99`): <!--# BB:Appreciation will be difficult for readers unless they are given an intuitive notion of what the scores mean (cf my comment above; what the figures focus on is essentially how quickly the maximum is reached and if so, that should be said explicitly and linked to the degree to which the resultant matrix will deviate from the data in the end. Could you make this all clearer here? :: ANNA Hope this has improved?-->

```{r densify_score 10, ones and zeros, echo = TRUE,  fig.show="hold", out.width="40%", fig.width=4, fig.height=2.5}
optimum11111 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111, 
                              exponent_prop_coded_data = 1, 
                              exponent_available_data_points = 1, 
                              exponent_lowest_taxon_coding_score = 1, 
                              exponent_lowest_variable_coding_score = 1, 
                              exponent_taxonomic_diversity = 1,
                              plot = TRUE)

optimum11000 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111, 
                              exponent_prop_coded_data = 1, 
                              exponent_available_data_points = 1, 
                              exponent_lowest_taxon_coding_score = 0, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 0,
                              plot = TRUE)

optimum11001 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111, 
                              exponent_prop_coded_data = 1,
                              exponent_available_data_points = 1, 
                              exponent_lowest_taxon_coding_score = 0, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 1,
                              plot = TRUE)

optimum11101 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111,
                              exponent_prop_coded_data = 1, 
                              exponent_available_data_points = 1, 
                              exponent_lowest_taxon_coding_score = 1, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 1, 
                              plot = TRUE)
```

The logical parameter `plot` is set to `TRUE` here, such that quality score plots are produced upon execution of the function. The default setting is `FALSE`. Appreciate from the plots how the quality score per iteration varies drastically depending on the parameter settings for `densify_score`. Accordingly, so do the optima:

```{r densify_score 12 comparison, echo = TRUE}
# compare the optima (number of iterations)
optimum11111 # for exponents 1, 1, 1, 1, 1
optimum11000 # for exponents 1, 1, 0, 0, 0
optimum11001 # for exponents 1, 1, 0, 0, 1
optimum11101 # for exponents 1, 1, 1, 0, 1
```

```{r densify_score 21, twos and ones, echo = FALSE, fig.show='hide'}
optimum12000 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111, 
                              exponent_prop_coded_data = 1,
                              exponent_available_data_points = 2,
                              exponent_lowest_taxon_coding_score = 0, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 0)

optimum13000 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111,
                              exponent_prop_coded_data = 1,
                              exponent_available_data_points = 3, 
                              exponent_lowest_taxon_coding_score = 0, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 0)

optimum21000 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111,
                              exponent_prop_coded_data = 2, 
                              exponent_available_data_points = 1, 
                              exponent_lowest_taxon_coding_score = 0, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 0)

optimum31000 <- densify_score(iteration_log_logodds_taxtrue_099099_seed1111,
                              exponent_prop_coded_data = 3, 
                              exponent_available_data_points = 1,
                              exponent_lowest_taxon_coding_score = 0, 
                              exponent_lowest_variable_coding_score = 0, 
                              exponent_taxonomic_diversity = 0)
```

Compare also how the optimum changes when varying the relative weight of exponents:

```{r densify_score comparison, echo = TRUE}
# compare the optima (number of iterations)
optimum11000 # for exponents 1, 1, 0, 0, 0
optimum12000 # for exponents 1, 2, 0, 0, 0
optimum13000 # for exponents 1, 3, 0, 0, 0
optimum21000 # for exponents 2, 1, 0, 0, 0
optimum31000 # for exponents 3, 1, 0, 0, 0
```

## 2.3. Matrix densification: retrieving the pruned matrix using `densify_prune`

The third function to complete matrix densification is given by `densify_prune`. It subsets the original data frame (parameter `original_data`) to the pruned data frame, given the outputs of `densify_steps` (parameter `iteration_log`) and `densify_score` (parameter `optimum`).

```{r densify_prune, echo = TRUE}
pruned_wals_exponents11111 <- 
  densify_prune(original_data = wals, 
                iteration_log = iteration_log_logodds_taxtrue_099099_seed1111, 
                optimum = optimum11111)

pruned_wals_exponents11001 <- 
  densify_prune(original_data = wals, 
                iteration_log = iteration_log_logodds_taxtrue_099099_seed1111, 
                optimum = optimum11001)
```

Compare the resulting data frames with the initial data frame.

The densified matrix resulting from setting all exponents to 1 encompasses 132 languages in 54 families coded for 97 variable at an overall coding density of 85.6%.
```{r densify_prune comparison 1, echo = TRUE}
# number of languages, families and variables, overall coding density
nrow(pruned_wals_exponents11111) 
taxonomy_matrix %>% 
  filter(id %in% rownames(pruned_wals_exponents11111)) %>% 
  select(level1) %>% unique() %>% nrow()
ncol(pruned_wals_exponents11111)
sum(!is.na(pruned_wals_exponents11111))/
  (ncol(pruned_wals_exponents11111)*nrow(pruned_wals_exponents11111))
```

Note how both language and variable coding densities have become very high, and how the inclusion of taxonomic diversity in both `densify_steps` and `densify_score` result in the inclusion of a number of languages that are not densely coded, but strongly contribute to taxonomic diversity and are thus kept.

```{r densify_prune plots, echo = FALSE, fig.show="hold", out.width="40%", fig.width=5, fig.height=3}
par(mar = c(4, 4, .1, .1))

ggplot(data.frame(density=apply(pruned_wals_exponents11111, 1, function(x) (length(na.omit(x))))/ncol(pruned_wals_exponents11111)), aes(x=density))+
  geom_histogram(color="black", fill="cadetblue2", bins=20)+
  theme_bw()+
  labs(title="Pruned WALS, log_odds, 0.99, 0.99,\nAll exponents = 1", 
       x="coding density per row (language)",
       y="frequency")

ggplot(data.frame(density=apply(pruned_wals_exponents11111, 2, function(x) (length(na.omit(x))))/nrow(pruned_wals_exponents11111)), aes(x=density))+
  geom_histogram(color="black", fill="forestgreen", bins=20)+
  theme_bw()+
  labs(title="Pruned WALS, log_odds, 0.99, 0.99,\nAll exponents = 1",
       x="coding density per column (variable)",
       y="frequency")
```

The densified matrix resulting from setting all exponents but `exponent_lowest_taxon_coding_score` and `exponent_lowest_variable_coding_score` to `1` encompasses 1452 languages in 232 families coded for 181 variables at an overall coding density of 26.2%.

```{r densify_prune comparison 2, echo = TRUE}
# number of languages, families and variables, overall coding density
nrow(pruned_wals_exponents11001) 
taxonomy_matrix %>% 
  filter(id %in% rownames(pruned_wals_exponents11001)) %>%
  select(level1) %>% unique() %>% nrow()
ncol(pruned_wals_exponents11001)
sum(!is.na(pruned_wals_exponents11001))/(ncol(pruned_wals_exponents11001)*nrow(pruned_wals_exponents11001))
```

```{r densify_prune 2 plots, echo = FALSE, fig.show="hold", out.width="40%", fig.width=5, fig.height=3}
par(mar = c(4, 4, .1, .1))

ggplot(data.frame(density=apply(pruned_wals_exponents11001, 1, function(x) (length(na.omit(x))))/ncol(pruned_wals_exponents11001)), aes(x=density))+
  geom_histogram(color="black", fill="cadetblue2", bins=20)+
  theme_bw()+
  labs(title="Pruned WALS, log_odds, 0.99, 0.99,\nExponents = c(1, 1, 0, 0, 1)", 
       x="coding density per row (language)",
       y="frequency")

ggplot(data.frame(density=apply(pruned_wals_exponents11001, 2, function(x) (length(na.omit(x))))/nrow(pruned_wals_exponents11001)), aes(x=density))+
  geom_histogram(color="black", fill="forestgreen", bins=20)+
  theme_bw()+
  labs(title="Pruned WALS, log_odds, 0.99, 0.99,\nExponents = c(1, 1, 0, 0, 1)",
       x="coding density per column (variable)",
       y="frequency")
```


# 3. Comparing outputs using varying parameters

To illustrate in more detail the effects of varying parameters in `densify_steps` and `densify_score`, we set 10 different exponent settings in `densify_score` for each of the 16 parameter settings specified above for `densify_steps` and log for each of the resulting matrices the overall coding density of the matrix as well as the number of languages, the number of families and the number of variables maintained. Each of these values are compared to the values of the original data frame. The full log-file is stored as a separate file (varying_parameters.csv).

```{r comparing outputs, echo = FALSE, eval = FALSE}
iteration_log_files <- c("iteration_log_arithmetic_taxfalse_seed4321","iteration_log_arithmetic_taxtrue_seed1234","iteration_log_geometric_taxfalse_seed8765","iteration_log_geometric_taxtrue_seed5678","iteration_log_logodds_taxfalse_050_seed6666","iteration_log_logodds_taxfalse_090_seed7777","iteration_log_logodds_taxfalse_095_seed8888","iteration_log_logodds_taxfalse_099_seed9999","iteration_log_logodds_taxtrue_050050_seed4444","iteration_log_logodds_taxtrue_090090_seed3333","iteration_log_logodds_taxtrue_090099_seed3311","iteration_log_logodds_taxtrue_095095_seed2222","iteration_log_logodds_taxtrue_095099_seed2211","iteration_log_logodds_taxtrue_099090_seed1133","iteration_log_logodds_taxtrue_099095_seed1122","iteration_log_logodds_taxtrue_099099_seed1111")

seeds <- c(4321,1234,8765,5678,6666,7777,8888,9999,4444,3333,3311,2222,2211,1133,1122,1111)

meantype <- c("arithmetic","arithmetic","geometric","geometric",
              "log_odds","log_odds","log_odds","log_odds",
              "log_odds","log_odds","log_odds","log_odds",
              "log_odds","log_odds","log_odds","log_odds")

taxtf <- c(F,T,F,T,
           F,F,F,F,
           T,T,T,T,
           T,T,T,T)

taxwf <- c(NA,NA,NA,NA,
           NA,NA,NA,NA,
           0.5,0.9,0.9,0.95,
           0.95,0.99,0.99,0.99)

codingwf <- c(NA,NA,NA,NA,
              0.5,0.9,0.05,0.99,
              0.5,0.9,0.99,0.95,
              0.99,0.9,0.95,0.99)

exponents <- matrix(data = c(1,1,1,1,1,
                             1,1,0,0,0,
                             1,1,0,0,1,
                             1,1,1,0,1,
                             1,1,1,0,0,
                             0.5,1,0,0,0,
                             0.5,1,0,0,0.5,
                             0.5,1,0,0,1,
                             0.5,0.5,0,0,1,
                             0.5,0.5,0.5,0,1),
                    nrow = , 
                    ncol = 5, 
                    byrow = T)

variability_threshold = 3

full_coding_proprotion = sum(!is.na(wals))/(ncol(wals)*nrow(wals))
full_n_lg = nrow(wals)
full_n_fam = taxonomy_matrix %>% filter(id %in% rownames(wals)) %>% select(level1) %>% unique() %>% nrow()
full_n_var = ncol(wals)

logfile <- data.frame(mean.type = "original", variability_threshold = NA, taxonomy = NA, seed = NA, taxonomy_weight = NA, coding_weight = NA, iteration_log_id = NA, exponent_prop_coded_data = NA, exponent_available_data_points = NA, exponent_lowest_taxon_coding_score = NA, exponent_lowest_variable_coding_score = NA, exponent_taxonomic_diversity = NA, full_coding_proprotion = full_coding_proprotion, number_languages = full_n_lg, number_families = full_n_fam, number_variables = full_n_var, increase_factor_coding_proportion = 1, proportion_languages_kept = 1, proportion_families_kept = 1, proportion_variables_kept = 1)

for (i in 1:length(iteration_log_files)){
  
  df <- get(iteration_log_files[i])
  docname <- iteration_log_files[i]
  mn <- meantype[i]
  variability_threshold <- 3
  taxonomy <- taxtf[i]
  seed <- seeds[i]
  twf <- taxwf[i]
  cwf <- codingwf[i]
  
  for (k in 1:nrow(exponents)){
    e1 <- exponents[k,1]
    e2 <- exponents[k,2]
    e3 <- exponents[k,3]
    e4 <- exponents[k,4]
    e5 <- exponents[k,5]
    
    scoring <- densify_score(df, exponent_prop_coded_data = e1, exponent_available_data_points = e2, exponent_lowest_taxon_coding_score = e3, exponent_lowest_variable_coding_score = e4, exponent_taxonomic_diversity = e5)
    pruning <- densify_prune(wals, df, scoring)
    cprop <- sum(!is.na(pruning))/(ncol(pruning)*nrow(pruning))
    nlg <- nrow(pruning)
    nfam <- taxonomy_matrix %>% filter(id %in% rownames(pruning)) %>% select(level1) %>% unique() %>% nrow()
    nvar <- ncol(pruning)
    
    logfile <- rbind(logfile,c(mn, variability_threshold, taxonomy, seed, twf, cwf, docname,
                             e1, e2, e3, e4, e5, cprop, nlg, nfam, nvar, cprop/full_coding_proprotion, nlg/full_n_lg, nfam/full_n_fam, nvar/full_n_var))
  }
}

write.csv(logfile,"varying_parameters.csv")
```

```{r read varying parameters, echo = FALSE}
logfile <- read.csv("varying_parameters.csv") %>% select(-X)
```


```{r head, echo = TRUE}
str(logfile)
```

# References
